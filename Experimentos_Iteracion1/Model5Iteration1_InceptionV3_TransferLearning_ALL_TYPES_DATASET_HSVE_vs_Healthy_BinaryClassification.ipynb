{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "935e70d6",
   "metadata": {},
   "source": [
    "# 1. IMPOTACIÓN DE LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acda23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Librería para acceder a variables del sistema\n",
    "import random # Librería para generar números aleatorios\n",
    "import time # Librería para contabilizar la duración del entrenamiento de cada modelo de CNN\n",
    "import numpy as np # Librería empleada para operaciones de array\n",
    "from os import path # Librería empleada para comprobar que existen ciertos archivos y directorios\n",
    "import pandas as pd # Para gestionar datos por medio de dataframes\n",
    "from os import listdir # Librería del sistema empleada para leer archivos o directorios\n",
    "import seaborn as sns # Librería para realizar mapas de calor (heatmap) con matrices de confusión con sus porcentajes\n",
    "import tensorflow as tf # Librería de Machine Learning\n",
    "from sklearn import metrics # Para obtener métricas de los modelos de clasificación\n",
    "from  matplotlib import image # Librería empleada para importar una imagen como un vector\n",
    "from skimage import transform # Para modificar las dimensiones de las imágenes y así unificarlas\n",
    "from keras import backend as K # backend from keras\n",
    "import matplotlib.pyplot as plt # Para generar gráficos y figuras\n",
    "from tensorflow import keras as k # Módulo de keras de tensorflow\n",
    "from keras.utils import plot_model # Para generar un diagrama con la arquitectura del modelo de CNN construido\n",
    "from keras.models import Sequential # Función de keras para inicializar un modelo secuencial\n",
    "from keras.utils import to_categorical # Función de keras para obtener la matriz codificada\n",
    "# mediante one-hot en el caso de la clasificación binaria\n",
    "from keras.callbacks import EarlyStopping # Módulo de Early Stopping de keras\n",
    "from sklearn.model_selection import train_test_split # Empleado para dividir los datos de entrenamiento en datos de entrenamiento y de validación\n",
    "from tensorflow.keras.optimizers import Adam # Se importa el optimizador adam\n",
    "from keras.layers import Conv2D, Flatten, Dense, Input, AveragePooling2D, Dropout, MaxPooling2D # capas para añadir\n",
    "# en el modelo secuencial de keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler # Se importa el planificador del learning rate decay\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # '0' = Everything, '1' = Warnings, '2' = Errors, '3' = Fatal\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True # Permitir el uso de más memoria de gpu dependiendo de la demanda de la ejecución\n",
    "\n",
    "session = tf.compat.v1.Session(config=config) # Se establece la configuración previa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a53365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se muestran las unidades de CPU y GPU disponibles en el entorno actual\n",
    "print(\"Num CPUs disponibles: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"CPU disponible: \", tf.config.list_physical_devices('CPU'))\n",
    "\n",
    "print(\"Num GPUs disponibles: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU disponible: \", tf.config.list_physical_devices('GPU'))\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50a3b1ec",
   "metadata": {},
   "source": [
    "# 2. INICIALIZACIÓN DE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se inicializan a una lista vacía las variables donde se almacenarán tanto las imágenes como las etiquetas de cada conjunto, es decir, de los conjuntos de entrenamiento, validación y test\n",
    "train_images = [] # list where images for training will be saved\n",
    "train_targets = [] # list where labels for the corresponding training images will be saved\n",
    "\n",
    "val_images = [] # list where images for val will be saved\n",
    "val_targets = [] # list where labels for the corresponding val images will be saved\n",
    "\n",
    "test_images = [] # list where images for test will be saved\n",
    "test_targets = [] # list where labels for the corresponding test images will be saved"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93fb6e95",
   "metadata": {},
   "source": [
    "# 3. LECTURA DE IMÁGENES DEL DATASET (TRAIN-VAL-TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae25788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubicación de las imágenes MRI de cerebros con HSVE\n",
    "\n",
    "class_HSVE_path = './DATASET/DATASET_Modelos1_2_3_4_5_6/HSVE_ORIGINAL_ALL_TYPES_RESCALED_256/' # Ubicación remota las imágenes MRI de la clase positiva (cerebros con la enfermedad HSVE)\n",
    "\n",
    "print('TRAIN PATH: ', class_HSVE_path) # Se visualiza el path de esta fracción del dataset (clase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubicación de las imágenes MRI de cerebros sanos\n",
    "\n",
    "class_HealthyBrains_path = './DATASET/DATASET_Modelos1_2_3_4_5_6/HEALTHY_BRAINS_ALL_TYPES_RESCALED_256/' # Ubicación remota las imágenes MRI de la clase negativa (cerebros sanos)\n",
    "\n",
    "print('TRAIN PATH: ', class_HealthyBrains_path) # Se visualiza el path de esta fracción del dataset (clase0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a218d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de todas las imágenes MRI de cerebros con HSVE tanto del conjunto de entrenamiento como del conjunto\n",
    "# de validación y de test. Para cada imagen leida se convierten cada uno de los píxeles de la imagen al tipo\n",
    "# numérico de coma flotante 'float32', se aplica una normalización estándar sobre la imagen para que\n",
    "# cada una tenga una media de 0 y una desviación estándar de 1 y se asigna la etiqueta\n",
    "# de la clase en cuestión. En este caso la se asigna la etiqueta 1 al tratarse de imágenes MRI de cerebros con HSVE\n",
    "\n",
    "for folder1 in listdir(class_HSVE_path):\n",
    "    if path.isdir(class_HSVE_path + folder1):\n",
    "        for folder2 in listdir(class_HSVE_path + folder1):\n",
    "            if path.isdir(class_HSVE_path + folder1 + '/' + folder2):\n",
    "                for folder3 in listdir(class_HSVE_path + folder1 + '/' + folder2):\n",
    "                    if path.isdir(class_HSVE_path + folder1 + '/' + folder2 + '/' + folder3):\n",
    "                        for filename in listdir(class_HSVE_path + folder1 + '/' + folder2 + '/' + folder3):\n",
    "                            print(filename)\n",
    "                            if path.isfile(class_HSVE_path + folder1 + '/' + folder2 + '/' + folder3 + '/' + filename) and filename != '.DS_Store' and filename != '._.DS_Store':\n",
    "\n",
    "                                img = image.imread(class_HSVE_path + folder1 + '/' + folder2 + '/' + folder3 + '/' + filename)\n",
    "\n",
    "                                img = img.astype(np.float32) # Se convierte la imagen a punto flotante de 32 bits\n",
    "\n",
    "                                imagen_normalized = (img - np.mean(img)) / np.std(img) # Se aplica una normalización estándar a cada imagen\n",
    "\n",
    "                                if(folder1 == 'training_set'):\n",
    "\n",
    "                                    train_images.append(imagen_normalized)\n",
    "                                    train_targets.append(1)\n",
    "\n",
    "                                    print(\"IMAGEN TRAIN LEÍDA: \", filename, \" | LABEL: \", 1)\n",
    "\n",
    "                                if(folder1 == 'validation_set'):\n",
    "\n",
    "                                    val_images.append(imagen_normalized)\n",
    "                                    val_targets.append(1)\n",
    "\n",
    "                                    print(\"IMAGEN VALIDATION LEÍDA: \", filename, \" | LABEL: \", 1)\n",
    "\n",
    "                                if(folder1 == 'test_set'):\n",
    "\n",
    "                                    test_images.append(imagen_normalized)\n",
    "                                    test_targets.append(1)\n",
    "\n",
    "                                    print(\"IMAGEN TEST LEÍDA: \", filename, \" | LABEL: \", 1)\n",
    "\n",
    "                                print(type(imagen_normalized))\n",
    "                                print(\"SHAPE: \", imagen_normalized.shape)\n",
    "\n",
    "num_HSVE_train = len(train_images)\n",
    "num_HSVE_val = len(val_images)\n",
    "num_HSVE_test = len(test_images)\n",
    "print()\n",
    "print(\"Número de imágenes de ENTRENAMIENTO LEÍDAS de la clase 1:\", num_HSVE_train)\n",
    "print(\"Número de imágenes de VALIDACIÓN LEÍDAS de la clase 1:\", num_HSVE_val)\n",
    "print(\"Número de imágenes de TEST LEÍDAS de la clase 1:\", num_HSVE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de todas las imágenes MRI de cerebros SANOS tanto del conjunto de entrenamiento como del conjunto\n",
    "# de validación y de test. Para cada imagen leida se convierten cada uno de los píxeles de la imagen al\n",
    "# tipo numérico de coma flotante 'float32', se aplica una normalización estándar sobre la imagen para que cada\n",
    "# una tenga una media de 0 y una desviación estándar de 1 y se asigna la etiqueta\n",
    "# de la clase en cuestión. En este caso la se asigna la etiqueta 0 al tratarse de imágenes MRI de cerebros con HSVE\n",
    "\n",
    "# Para que las clases sean equilibradas, como máximo se leeran tantas imágenes MRI de cerebros sanos como número de\n",
    "# imágenes MRI de la enfermedad HSVE se hayan leido en la celda anterior\n",
    "# Se calculan las imágenes de HSVE leidas en traning, validación y test\n",
    "num_HSVE_train = len(train_images)\n",
    "num_HSVE_val = len(val_images)\n",
    "num_HSVE_test = len(test_images)\n",
    "num_HEALTHY_train = 0\n",
    "num_HEALTHY_val = 0\n",
    "num_HEALTHY_test = 0\n",
    "\n",
    "for folder1 in listdir(class_HealthyBrains_path):\n",
    "    if path.isdir(class_HealthyBrains_path + folder1):\n",
    "        for folder2 in listdir(class_HealthyBrains_path + folder1):\n",
    "            if path.isdir(class_HealthyBrains_path + folder1 + '/' + folder2):\n",
    "                for folder3 in listdir(class_HealthyBrains_path + folder1 + '/' + folder2):\n",
    "                    if path.isdir(class_HealthyBrains_path + folder1 + '/' + folder2 + '/' + folder3):\n",
    "                        for filename in listdir(class_HealthyBrains_path + folder1 + '/' + folder2 + '/' + folder3):\n",
    "                            print(filename)\n",
    "                            if path.isfile(class_HealthyBrains_path + folder1 + '/' + folder2 + '/' + folder3 + '/' + filename) and filename != '.DS_Store' and filename != '._.DS_Store':\n",
    "\n",
    "                                img = image.imread(class_HealthyBrains_path + folder1 + '/' + folder2 + '/' + folder3 + '/' + filename)\n",
    "\n",
    "                                img = img.astype(np.float32) # Se convierte la imagen a punto flotante de 32 bits\n",
    "\n",
    "                                imagen_normalized = (img - np.mean(img)) / np.std(img) # Se aplica una normalización estándar a cada imagen\n",
    "\n",
    "                                if(folder1 == 'training_set'):\n",
    "\n",
    "                                    if(num_HEALTHY_train == num_HSVE_train):\n",
    "                                        break\n",
    "\n",
    "                                    train_images.append(imagen_normalized)\n",
    "                                    train_targets.append(0)\n",
    "                                    num_HEALTHY_train = num_HEALTHY_train + 1\n",
    "\n",
    "                                    print(\"IMAGEN TRAIN LEÍDA: \", filename, \" | LABEL: \", 0)\n",
    "\n",
    "                                if(folder1 == 'validation_set'):\n",
    "\n",
    "                                    if(num_HEALTHY_val == num_HSVE_val):\n",
    "                                        break\n",
    "\n",
    "                                    val_images.append(imagen_normalized)\n",
    "                                    val_targets.append(0)\n",
    "                                    num_HEALTHY_val = num_HEALTHY_val + 1\n",
    "\n",
    "                                    print(\"IMAGEN VALIDATION LEÍDA: \", filename, \" | LABEL: \", 0)\n",
    "\n",
    "                                if(folder1 == 'test_set'):\n",
    "\n",
    "                                    if(num_HEALTHY_test == num_HSVE_test):\n",
    "                                        break\n",
    "\n",
    "                                    test_images.append(imagen_normalized)\n",
    "                                    test_targets.append(0)\n",
    "                                    num_HEALTHY_test = num_HEALTHY_test + 1\n",
    "\n",
    "                                    print(\"IMAGEN TEST LEÍDA: \", filename, \" | LABEL: \", 0)\n",
    "\n",
    "                                print(type(imagen_normalized))\n",
    "                                print(\"SHAPE: \", imagen_normalized.shape)\n",
    "\n",
    "print()\n",
    "print(\"Número de imágenes de ENTRENAMIENTO LEÍDAS de la clase 0:\", num_HEALTHY_train)\n",
    "print(\"Número de imágenes de VALIDACIÓN LEÍDAS de la clase 0:\", num_HEALTHY_val)\n",
    "print(\"Número de imágenes de TEST LEÍDAS de la clase 0:\", num_HEALTHY_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f368888b",
   "metadata": {},
   "source": [
    "# 4. PRE-PROCESADO DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12964dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean los arrays de las imágenes de entrada\n",
    "X_train = np.asarray(train_images)\n",
    "X_val = np.asarray(val_images)\n",
    "X_test = np.asarray(test_images)\n",
    "\n",
    "# Se crean los arrays de las etiquetas de entrada\n",
    "train_targets_array = np.asarray(train_targets)\n",
    "val_targets_array = np.asarray(val_targets)\n",
    "test_targets_array = np.asarray(test_targets)\n",
    "\n",
    "# # Codificación de las etiquetas para clasificación multiclase\n",
    "# # y_labels_train = to_categorical(train_targets_array) # enconding the train labels with the one_hot method\n",
    "# # y_labels_test = to_categorical(test_targets_array) # enconding the test labels with the one_hot method\n",
    "#\n",
    "# # Se divide el conjunto de entrenamiento en training y validation. Además, el parámetro stratify permite asegurar que la\n",
    "# # proporción de ejemplos en cada clase sea la misma tras haberse realizado la división\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, train_targets_array, test_size=0.21, shuffle=True, random_state=42, stratify=train_targets_array)\n",
    "\n",
    "print(\"Forma de los datos de ENTRENAMIENTO: \", X_train.shape) # data.shape = (289, 256, 256, 1)\n",
    "print(\"Forma de las etiquetas de ENTRENAMIENTO: \", train_targets_array.shape) # labels.shape = (289,)\n",
    "print()\n",
    "\n",
    "print(\"Forma de los datos de VALIDACIÓN: \", X_val.shape) # labels.shape = (51, 256, 256, 1)\n",
    "print(\"Forma de las etiquetas de VALIDACIÓN: \", val_targets_array.shape) # labels.shape = (51,)\n",
    "print()\n",
    "\n",
    "print(\"Forma de los datos de TEST: \", X_test.shape) # data.shape = (72, 256, 256, 1)\n",
    "print(\"Forma de las etiquetas de TEST: \", test_targets_array.shape) # labels.shape = (72,)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clases = 2\n",
    "\n",
    "contador_clases_training = np.bincount(train_targets_array) # Se cuenta el número de imágenes de cada clase en el conjunto de entrenamiento\n",
    "\n",
    "# Se imprime el número de imágenes de cada clase en el conjunto de entrenamiento\n",
    "for i in range(0, num_clases):\n",
    "    print(\"Número de ejemplos de la clase\",i, \" en el conjunto de entrenamiento:\", contador_clases_training[i])\n",
    "print()\n",
    "\n",
    "contador_clases_validation = np.bincount(val_targets_array) # Se cuenta el número de imágenes de cada clase en el conjunto de validación\n",
    "\n",
    "# Se imprime el número de imágenes de cada clase en el conjunto de validación\n",
    "for i in range(0, num_clases):\n",
    "    print(\"Número de ejemplos de la clase\",i, \" en el conjunto de validación:\", contador_clases_validation[i])\n",
    "print()\n",
    "\n",
    "contador_clases_test = np.bincount(test_targets_array) # Se cuenta el número de imágenes de cada clase en el conjunto de test\n",
    "\n",
    "# Se imprime el número de imágenes de cada clase en el conjunto de test\n",
    "for i in range(0, num_clases):\n",
    "    print(\"Número de ejemplos de la clase\",i, \" en el conjunto de test:\", contador_clases_test[i])\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c39ab2d",
   "metadata": {},
   "source": [
    "# 5. Previsualización de los datos de los 3 conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc83a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_clases = {\n",
    "    0: 'Healthy brains',\n",
    "    1: 'Brains with HSVE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f551ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsualización del training dataset\n",
    "\n",
    "# max_train = len(X_train)   # número máximo de imágenes del conjunto de entrenamiento\n",
    "# random_index_train = random.sample(range(max_train), 6)\n",
    "#\n",
    "# plt.figure(figsize=(16, 12))#Plots our figures\n",
    "# for index, i in enumerate(random_index_train):\n",
    "#     plt.subplot(2, 3, index+1)\n",
    "#     plt.imshow(X_train[i, :, :], cmap='gray')\n",
    "#\n",
    "#     etiqueta = train_targets_array[i]\n",
    "#     nombre_etiqueta = nombre_clases[etiqueta]\n",
    "#     plt.title('Etiqueta: {} - ({})'.format(etiqueta, nombre_etiqueta))\n",
    "#\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad509812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsualización del validation dataset\n",
    "\n",
    "# max_val = len(X_val)   # número máximo de imágenes del conjunto de validación\n",
    "# random_index_val = random.sample(range(max_val), 6)\n",
    "#\n",
    "# plt.figure(figsize=(16, 12))#Plots our figures\n",
    "# for index, i in  enumerate(random_index_val):\n",
    "#     plt.subplot(2, 3, index+1)\n",
    "#     plt.imshow(X_val[i, :, :], cmap='gray')\n",
    "#\n",
    "#     etiqueta = train_targets_array[i]\n",
    "#     nombre_etiqueta = nombre_clases[etiqueta]\n",
    "#     plt.title('Etiqueta: {} - ({})'.format(etiqueta, nombre_etiqueta))\n",
    "#\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7943375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsualización del test dataset\n",
    "\n",
    "# max_test = len(X_test)   # número máximo de imágenes del conjunto de test\n",
    "# random_index_test = random.sample(range(max_test), 6)\n",
    "#\n",
    "# plt.figure(figsize=(16, 12))#Plots our figures\n",
    "# for index, i in  enumerate(random_index_test):\n",
    "#     plt.subplot(2, 3, index+1)\n",
    "#     plt.imshow(X_test[i, :, :], cmap='gray')\n",
    "#\n",
    "#     etiqueta = train_targets_array[i]\n",
    "#     nombre_etiqueta = nombre_clases[etiqueta]\n",
    "#     plt.title('Etiqueta: {} - ({})'.format(etiqueta, nombre_etiqueta))\n",
    "#\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. DEFINICIÓN Y CONFIGURACIÓN DEL MODELO DE CNN (Red Neuronal Convolucional) con Transfer Learning del modelo InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a InceptionV3 model with pre-trained weights\n",
    "base_model = k.applications.InceptionV3(weights='imagenet', include_top=False)\n",
    "base_model.layers.pop(0)\n",
    "\n",
    "base_model.summary()\n",
    "\n",
    "# Save the model defined previously\n",
    "\n",
    "plot_model(base_model, to_file='arquitecturas_modelos/Model5Iteration1_baseModel_InceptionV3.png', show_shapes=True)\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# input_layer = Input(shape=(256, 256, 1))\n",
    "# x = Conv2D(3, (3, 3), padding='same', activation='relu')(input_layer)\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    # Define the Conv2D layer\n",
    "    input_layer = Conv2D(3, kernel_size = 3, activation = 'relu', input_shape=(256, 256, 1))\n",
    "\n",
    "    # creating Keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    # new input layer\n",
    "    model.add(input_layer)\n",
    "    # model transfer learning\n",
    "    model.add(base_model)\n",
    "    # Feature learning part\n",
    "    # model.add(Conv2D(32, kernel_size = 3, activation = 'relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(Conv2D(64, kernel_size = 3, activation = 'relu'))\n",
    "    # model.add(Dropout(0.3))\n",
    "    # model.add(AveragePooling2D(pool_size = (3, 3), strides = (3,3)))\n",
    "    # model.add(AveragePooling2D(pool_size = (3, 3), strides = (3, 3)))\n",
    "    # Classifier part\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "122b9259",
   "metadata": {},
   "source": [
    "# 7. ENTRENAMIENTO DEL MODELO (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configuración de la compilación del modelo anterior:\n",
    "# Se planifica el learning rate mediante la implementación 'Exponential Decay' y se emplea el optimizador 'Adam'\n",
    "\n",
    "# lr_schedule = k.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=1e-6)\n",
    "# adam_optimizer = k.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Se muestra un resumen de la arquitectura del modleo secuencial de keras definido en la celda anterior\n",
    "with tf.device('/GPU:0'):\n",
    "    model_gpu = get_model()\n",
    "    # showing a summary of the layers and parameters of the model created\n",
    "    model_gpu.summary()\n",
    "\n",
    "    # Save the model defined previously\n",
    "    plot_model(model_gpu, to_file='arquitecturas_modelos/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_arquitecture.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Se define un optimizador de tipo 'Adam' con el learning rate por defecto, es decir, con el valor 0.0001\n",
    "optimizer = Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se compila el modelo mediante la GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    model_gpu.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy','Precision'])\n",
    "    print(\"Modelo compilado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Se define otro callback de 'early stopping' que permita detener el entrenamiento del modelo si el modelo no mejora el valor de su 'val_accuracy' en 5 epochs consecutivas\n",
    "early_stop_val_accuracy = EarlyStopping(monitor='val_accuracy', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Se define otro callback de 'early stopping' que permita detener el entrenamiento del modelo si el modelo no mejora el valor de su 'val_loss' en 5 epochs consecutivas\n",
    "early_stop_val_loss = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Define the learning rate scheduler function\n",
    "# def lr_scheduler(epoch, lr):\n",
    "#     # Set the initial learning rate\n",
    "#     lr_init = 0.0001\n",
    "#     # Set the decay rate\n",
    "#     decay_rate = 0.1\n",
    "#     # Calculate the new learning rate for this epoch\n",
    "#     lr_new = lr_init / (1.0 + decay_rate * epoch)\n",
    "#     # Print the new learning rate\n",
    "#     print('Epoch %d: Learning rate is %f' % (epoch+1, lr_new))\n",
    "#     # Return the new learning rate\n",
    "#     return lr_new\n",
    "#\n",
    "# # Define the learning rate scheduler callback\n",
    "# lr_callback = LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ENTRENA el modelo mediante la GPU\n",
    "# Se especifican una serie de hiperparámetros tales como las epochs (un máximo de 50), un batch size de 8 para que\n",
    "# # la GPU sea capaz de almacenar en memoria hasta 8 imágenes a la vez antes de actualizar los parámetros y se\n",
    "# # especifican los 2 callbacks previamente declarados\n",
    "with tf.device('/GPU:0'):\n",
    "\n",
    "    print(\"Entrenando modelo de CNN...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    h = model_gpu.fit(X_train, train_targets_array, epochs = 30, validation_data=(X_val, val_targets_array), batch_size = 8, callbacks=[early_stop_val_accuracy, early_stop_val_loss], verbose=1) # training method in which data and targets are passed with some specific parameters\n",
    "\n",
    "    print(\"Modelo entrenado\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(\"Duración del entrenamiento:\", int(training_time/60), \"minutos y\", int(training_time%60), \"segundos\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16ccfe19",
   "metadata": {},
   "source": [
    "# 8. EVALUACIÓN DEL MODELO (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tu código para la evaluación de la red neuronal de la pregunta 2 aquí ###\n",
    "\n",
    "# Se evalúa el modelo entrenado de red neuronal\n",
    "# pack = model_gpu.evaluate(X_test, test_targets_array)\n",
    "# print(pack)\n",
    "# print('Pérdida (loss) obtenida en el conjunto de prueba (test):', test_loss)\n",
    "# print('Exactitud (accuracy) obtenida en el conjunto de prueba (test):', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proceso de evaluación\n",
    "\"\"\"\n",
    "getting predictions with the trained model with the test data as input and\n",
    "getting the index of the max value in order to compare it with the labels\n",
    "\"\"\"\n",
    "\n",
    "# Evaluación para clasificación binaria con sigmoid\n",
    "\n",
    "etiquetas = test_targets_array\n",
    "rounded_preds = np.round(model_gpu.predict(X_test)).astype(int)\n",
    "lista_unica = np.array(rounded_preds).flatten().tolist()\n",
    "preds = [int(i) for i in lista_unica]\n",
    "results = etiquetas == preds\n",
    "\n",
    "print(etiquetas)\n",
    "print(preds)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proceso de evaluación\n",
    "# \"\"\"\n",
    "# getting predictions with the trained model with the test data as input and\n",
    "# getting the index of the max value in order to compare it with the labels\n",
    "# \"\"\"\n",
    "\n",
    "# Evaluación para clasificación multiclase con softmax\n",
    "# preds = np.argmax(model_gpu.predict(X_test))\n",
    "# labels = np.argmax(test_targets_array) # getting the index of the max value of each column to compare it with the predictions in order to get pred results\n",
    "# results = preds == labels\n",
    "#\n",
    "#\n",
    "# print(preds)\n",
    "# print(labels)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtienen estadísticas sencillas a partir de los resultados obtenidos en las predicciones\n",
    "\n",
    "correct = np.sum(results == True)\n",
    "incorrect = np.sum(results == False)\n",
    "print(\"Correct: \", correct, \" Correct Acc: \", (correct/len(results))*100)\n",
    "print(\"Incorrect: \", incorrect, \" Incorrect Acc: \", (incorrect/len(results))*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "708a5067",
   "metadata": {},
   "source": [
    "# 9. ANÁLISIS DE LOS RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene la tabla de métricas de clasificación con la precision, F1-score, recall y accuracy\n",
    "\n",
    "target_names = ['Cerebro sano', 'Cerebro con HSVE']\n",
    "report = metrics.classification_report(etiquetas, preds, target_names=target_names, output_dict=True)\n",
    "print(report)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Se alm el número de imágenes de cada clase en el conjunto de entrenamiento\n",
    "for i in range(0, num_clases):\n",
    "    df.at[\"TRAINING - \"+str(i), 0] = \"Número de ejemplos de la clase \" + str(i) + \" en el conjunto de entrenamiento: \" + str(contador_clases_training[i])\n",
    "\n",
    "# Se imprime el número de imágenes de cada clase en el conjunto de validación\n",
    "for i in range(0, num_clases):\n",
    "    df.at[\"VALIDACIÓN - \"+str(i), 0] = \"Número de ejemplos de la clase \" + str(i) + \" en el conjunto de validación: \" + str(contador_clases_validation[i])\n",
    "\n",
    "# Se imprime el número de imágenes de cada clase en el conjunto de test\n",
    "for i in range(0, num_clases):\n",
    "    df.at[\"TEST - \"+str(i), 0] = \"Número de ejemplos de la clase \" + str(i) + \" en el conjunto de test: \" + str(contador_clases_test[i])\n",
    "\n",
    "df.at[\"Training-time\", 0] = \"Duración del entrenamiento: \" + str(int(training_time/60)) + \" minutos y \" + str(int(training_time%60)) +  \" segundos\"\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = metrics.accuracy_score(etiquetas, preds)\n",
    "print(\"Model Accuracy: \", accuracy)\n",
    "df.at[\"Accuracy\", 0] = \"Model Accuracy: \" + str(accuracy)\n",
    "\n",
    "# ROC AUC\n",
    "predicted_proba = model_gpu.predict(X_test) # Se obtienen las probabilidades de predicción del modelo\n",
    "auc = metrics.roc_auc_score(etiquetas, predicted_proba) # Se calcula el AUC-ROC\n",
    "print('AUC-ROC:', auc)\n",
    "df.at[\"AUC-ROC\", 0] = \"AUC-ROC: \" + str(auc)\n",
    "\n",
    "df.to_csv('classification_reports/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_classification_report.csv', index=True)\n",
    "\n",
    "# Para multiclase\n",
    "# predicted_proba = model_gpu.predict_proba(X_test)\n",
    "# auc = metrics.roc_auc_score(etiquetas, predicted_proba, multi_class='ovr')\n",
    "# print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d971a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se visualiza la matriz de confusión\n",
    "confusion_matrix = tf.math.confusion_matrix(preds, etiquetas)\n",
    "cm = plt.figure(figsize=(10, 7))\n",
    "\n",
    "\n",
    "x_axis_labels = ['Cerebro sano', 'Cerebro con HSVE'] # labels for x-axis [0, 1]\n",
    "y_axis_labels = ['Cerebro sano', 'Cerebro con HSVE'] # labels for y-axis [0, 1]\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "heat_map = sns.heatmap((confusion_matrix/np.sum(confusion_matrix, axis=1, keepdims=True)), annot=True, cmap=\"Blues\",  xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "# Si no funciona, entonces probar con cmap=colormap, definido arriba\n",
    "\n",
    "font1 = {'family':'serif','color':'blue','size':20}\n",
    "font2 = {'family':'serif','color':'darkred','size':20}\n",
    "\n",
    "plt.title(\"Matriz de Confusión\", fontdict = font1)\n",
    "plt.ylabel('Valores predichos', fontdict = font2)\n",
    "plt.xlabel('Valores reales', fontdict = font2)\n",
    "\n",
    "fig = heat_map.get_figure()\n",
    "fig.savefig('figuras/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeff9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtienen las métricas de exactitud y pérdida obtenida en training y validation en cada epoch del entrenamiento\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "font1 = {'family':'serif','color':'blue','size':20}\n",
    "font2 = {'family':'serif','color':'darkred','size':20}\n",
    "\n",
    "figure = pd.DataFrame(h.history).plot(figsize=(12,8))\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "plt.title(\"Accuracy vs loss\", fontdict = font1)\n",
    "plt.xlabel(\"Epochs\", fontdict = font2)\n",
    "\n",
    "fig = figure.get_figure()\n",
    "fig.savefig('figuras/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_accuracy_vs_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Se obtiene la exactitud o accuracy obtenida en training y validation en cada epoch del entrenamiento\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "#accuracy per epoch graph\n",
    "accuracy = h.history['accuracy']\n",
    "val_accuracy = h.history['val_accuracy']\n",
    "epochs = h.epoch\n",
    "figure = plt.figure(figsize=(6,8))\n",
    "plt.title('Evolución de la exactitud (accuracy)', fontdict = font1)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.xlabel(\"Epochs\", fontdict = font2)\n",
    "plt.plot(epochs, accuracy)\n",
    "plt.plot(epochs, val_accuracy)\n",
    "\n",
    "# figure = pd.DataFrame(h.history).plot(figsize=(12,8))\n",
    "\n",
    "fig = figure.get_figure()\n",
    "fig.savefig('figuras/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85730934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene la pérdida o loss obtenida en training y validation en cada epoch del entrenamiento\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "epochs = h.epoch\n",
    "figure = plt.figure(figsize=(6,8))\n",
    "plt.title('Evolución de la pérdida (loss)', fontdict = font1)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.xlabel(\"Epochs\", fontdict = font2)\n",
    "plt.plot(epochs,loss)\n",
    "plt.plot(epochs,val_loss)\n",
    "\n",
    "fig = figure.get_figure()\n",
    "fig.savefig('figuras/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se visualiza la salida de las activaciones intermedias\n",
    "\n",
    "# get_linear_filters = K.function(inputs=model_gpu.layers[0].input, outputs=model_gpu.layers[1].output)\n",
    "# get_activations = K.function(inputs=model_gpu.layers[0].input, outputs=model_gpu.layers[1].output)\n",
    "# filters_applied = get_linear_filters([X_train[0:2, :, :, :],0])\n",
    "# activations_output = get_activations([X_train[0:2, :, :, :],0])\n",
    "#\n",
    "#\n",
    "# filters = plt.figure(figsize=(8,8))\n",
    "# for i in range(32):\n",
    "#     ax = filters.add_subplot(6, 6, i + 1)\n",
    "#     ax.imshow(filters_applied[0][:, :, i], cmap = 'gray')\n",
    "#     plt.xticks(np.array([]))\n",
    "#     plt.yticks(np.array([]))\n",
    "#     plt.tight_layout()\n",
    "#\n",
    "# filters.savefig('20_filters_applied')\n",
    "#\n",
    "# activations = plt.figure(figsize=(8,8))\n",
    "# for i in range(32):\n",
    "#     ax = activations.add_subplot(6, 6, i + 1)\n",
    "#     ax.imshow(activations_output[0][:, :, i], cmap = 'gray')\n",
    "#     plt.xticks(np.array([]))\n",
    "#     plt.yticks(np.array([]))\n",
    "#     plt.tight_layout()\n",
    "\n",
    "# activations.savefig('figuras/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_20_activations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c60a0d00",
   "metadata": {},
   "source": [
    "# 10. EXPORTACIÓN DEL MODELO E HISTÓRICOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se exporta el historial con la precisión y pérdida del modelo guardado\n",
    "np.save('modelos_entrenados/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_HSVE_vs_Healthy_HISTORY.npy',h.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Se guarda/exporta el modelo de CNN entrenado\n",
    "model_gpu.save('modelos_entrenados/Model5Iteration1_InceptionV3_TransferLearning_ALL_TYPES_DATASET_HSVE_vs_Healthy.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
